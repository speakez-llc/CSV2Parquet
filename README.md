# CSV2Parquet console utility

This console app reads through a series of CSV files and performs column and row-level enrichments before outputting to parquet format. Part of the goal is to make the data more transportable. There's a level of "natural" compression that occurs with the standard "Snappy" compression - aside from the parquet format itself - being a mix of column and row-store to reduce unnecessary repetition of values in files. In the case here with CMS Medicare Part D data the ratio was about 6:1 reduction - from 32GB of data to about 5.9 or so before adding calculated columns. With the new calculated columns, which have a high cardinality - the reduction was closer to 3:1. But any reduced multiple is a good multiple when it comes to data movement - and if I was more sensitive to it I would do more to extract dimensional data and GZip the files on output. Just imposing order to this level was good enough for this project and scope of data.

## DuckDB

The choice to use [DuckDB.NET](<https://github.com/Giorgi/DuckDB.NET>) over a library like [ParquetSharp](<https://github.com/G-Research/ParquetSharp>) (which wraps the Apache Parquet C++ library) was based the direct approach of using its PostgreSQL-like queries to perform the transforms. The process is easy to understand, even if you don't have a great deal of experience with F# conventions. The *interesting* work is performed by SQL.

## The Data

This project takes ten years of Medicare Part D data and transforms it into a light-weight "star" model for reporting. It's loosely based on [this Azure Fabric demo repo](<https://github.com/isinghrana/fabric-samples-healthcare>) which uses Jupyter notebooks to create the store of data that's sourced by the PowerBI dashboards. Between the Spark errors we observed while trying to follow their process, and the fact that they stripped down the data into a form that was just a sub-set of the original corpus, we opted to use this process to pull in the full data set and apply some of our own transforms to smooth a few rough edges and provide a broader palette of information on which to derive insights. One such case was adding an *IS_OPIOID* boolean column where the medication name was checked against a list of opioid pharmaceuticals.

While the relative quality of this [CMS data set](<https://data.cms.gov/provider-summary-by-type-of-service/medicare-part-d-prescribers/medicare-part-d-prescribers-by-provider-and-drug>) could be a "TED Talk" in and of itself, the purpose of this project is to show, as simply as possible, how CSV data can be easily wrangled using the DuckDB.NET library. While I preserved the references to those files in the code I removed them from the project so I wouldn't get caught up pushing 32GB of CSVs into GitHub LFS. ðŸ˜‰ The data is available in the link above if you're interested in working with that data using these "breadcrumbs" here. There *is* a sample CSV file, referenced below to give interested viewers a "TL;DR" view of the data as provided by CMS.

### Sample Data sidebar

When building out this process initially, we needed faster initial load of data to check the process and ensure that the output was right. So we generated a CSV with a sub-set of rows from one of the files using a PowerShell script that would parse the first 1,000 rows of the named file and then pull a semi-random selection from the remaining rowset. There's any number of ways to accomplish this task, but this was the one that made sense at the time so it is included here [sample_CMS.ps1](src/CSV2Parquet/sample_CMS.ps1) and [sample.csv](src/CSV2Parquet/sample.csv) here as a breadcrumb for what that early exploratory part of the process looked like.

### The bigger picture - DuckDB as a source database in IDEs

There's a few additional "windows of opportunity" beyond simple file processing. For longer-form data-centric projects the DX experience can go further by mounting those generated files as a DuckDB instance and make it available to your editor/IDE of choice. Both VSCode and JetBrains Rider have DuckDB connection extensions that allow you to query that data within the environment.

![alt text](<img/Screenshot 2024-08-11 195048.png>)

But beyond that, that connection can also be used to provide syntax/type checks for those Postgres-like queries that are in-inlined in the code. So you can essentially treat that "flat" data as any other data store.

![alt text](<img/Screenshot 2024-08-13 130130.png>)

### The even BIGGER picture

While in our case it was more convenient to create this as a local console app on a workstation, this code could very easily drop into a [.NET Interactive Notebook](<https://github.com/dotnet/interactive>) such as with the [Polyglot Notebook](https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.dotnet-interactive-vscode) extension for VSCode. While the sample project that inspired this work used "standard" Jupyter notebooks our bias is to avoid the pitfalls of python and Scala/Spark altogether, so if a project ever called for the interactivity and exploration of a notebook environment, the polyglot notebook format would be our choice.  

### Visual storytelling

To familiarize ourselves with the CMS data we created a PowerBI dashboard that approximates the report shown in the original sample project. The historical data had some surprises in it, and will be something we'll explore more deeply with our own reporting framework. Such as it is, this provides a useful first-blush view of the data.

![alt text](<img/Screenshot 2024-08-12 205619.png>)

As mentioned above. If a notebook approach was used, many of these visuals could be built in-line and the data exploration could proceed from there. In either case, DuckDB and the .NET wrapper we've used provides a fast, efficient method of inspecting and processing unstructured data into a compact and useful format.
